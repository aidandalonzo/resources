{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are the workhorse of breakthroughs in machine learning in the last decade. The reason for this is that they are able to learn complex non-linear relationships between inputs and outputs. In this notebook, we will explore the basics of neural networks and how to implement them in Python.\n",
    "\n",
    "For simplicity, we'll use so-called fully connected deep neural networks. It sounds mysterious, but it's just a function, given by \n",
    "\n",
    "$y = f(x) = \\sigma(W_2 \\sigma(W_1 x + b_1) + b_2)$\n",
    "\n",
    "where $x$ is the input, $y$ is the output, $W_1$ and $W_2$ are matrices of weights, $b_1$ and $b_2$ are vectors of biases, and $\\sigma$ is a non-linear function called the activation function. The activation function is applied element-wise to its input.  The number of layers in the network is the depth of the network, and the number of neurons in each layer is the width of the network. The number of neurons in the input layer is the dimensionality of the input, and the number of neurons in the output layer is the dimensionality of the output.\n",
    "\n",
    "The weights and biases are the parameters of the neural network. When the neural network is initialized on your computer these are drawn randomly from a distribution, but the process of machine learning involves some optimization process on the parameters to achieve some goal -- winning at chess, generating fake images of galaxies, classifying phases of matter, etc etc.  Parameters that are not updated during learning are called hyperparameters. The depth and width of the network are hyperparameters in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax\n",
    "\n",
    "The libraries we'll use for neural networks are `jax` + `flax`. `jax` is a library for automatic differentiation and vectorization, and `flax` is a library for neural networks built on top of `jax`.  `jax` is a bit like `numpy` but with automatic differentiation.  Other very popular neural network libraries include `tensorflow` and `pytorch`.\n",
    "\n",
    "There are a few essential functions / features in jax that you have to get used to before you worry about neural networks. They are \n",
    "- `jax.numpy` which is a version of `numpy` that can be differentiated\n",
    "- `jax.vmap` which is a function that vectorizes a function\n",
    "- `jax.grad` which is a function that computes the gradient of a function\n",
    "- `jax.jit` which is a function that compiles a function for faster execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out `grad`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sin(2*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_sin = grad(jnp.sin) # takes the gradient of sin\n",
    "print(grad_sin(2*math.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want to compute the gradient of a function that takes a vector as input?  Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_vec_input = vmap(grad_sin) # takes the gradient of sin for each element in the input\n",
    "print(grad_vec_input(jnp.array([0., math.pi/4, math.pi/2, 3*math.pi/4, math.pi])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates `vmap`, which is a function that vectorizes a function. Note I can't use it on a single input. Can I use it on a vector with a slightly different shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_vec_input(jnp.array([2*math.pi]))\n",
    "print(grad_vec_input(jnp.array([[0.], [math.pi/4], [math.pi/2],[ 3*math.pi/4], [math.pi/2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? One gives a scalar on this input, the other technically does not even though it has the same numbers in it. It's about the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = jnp.array([[0.], [math.pi/4], [math.pi/2],[ 3*math.pi/4], [math.pi/2]])\n",
    "print(my_array.shape)\n",
    "print(my_array.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "squeezed_sin = lambda x: jnp.sin(x).squeeze()\n",
    "grad_vec_input_take2 = vmap(grad(squeezed_sin))\n",
    "print(grad_vec_input_take2(my_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vmap knows what to do with the first index. It's the second index that is the issue here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Okay, jax is just faster?!** Is that the reason to use it? Yes, `jax` is a bit faster if you are dealing with large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vec_input = vmap(jnp.sin)\n",
    "a_million_random_numbers = np.random.uniform(0, 2*math.pi, (1000000,))\n",
    "vec_input(a_million_random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "other_way_to_compute = [jnp.sin(x) for x in a_million_random_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Neural Network: Linear Regression\n",
    "\n",
    "A neural network is just a function with parameters!? That means we know them from middle school: \n",
    "\n",
    "$y=mx + b$ \n",
    "\n",
    "is a function with parameters. This is an extremely simple neural network, and we wish to remind ourselves of the basic training scheme, using `jax` to do linear regression.\n",
    "\n",
    "First we generate some noise data around the line\n",
    "\n",
    "$y=10 x + 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 50\n",
    "\n",
    "num_points = 25\n",
    "\n",
    "xs = np.random.normal(size=(num_points,1))\n",
    "noise = np.random.normal(scale=1.,size=(num_points,1))\n",
    "ys = 10*xs + 4 + noise\n",
    "\n",
    "plt.scatter(xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we didn't generate this data ourselves, instead that some experiment gave it to us. We want to fit a model to it. In this case we know \n",
    "\n",
    "$f(x) = mx + b$ \n",
    "\n",
    "is a good model, but a prior in an experimental setup we wouldn't know that. Still, let's proceed, by defining the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta, x):\n",
    "    m, b = theta\n",
    "    return m*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for any specific values of $m$ and $b$, $f(x)$ makes predictions, and we want to know whether those predictions are good or bad relative to the ground truth values encoded in the variable `ys`. For that, we need a loss function, we'll use mean-squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(theta, x, y):\n",
    "    return jnp.mean((f(theta,x) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train this function to make better predictions, i.e. we want to move in parameter space to make the predictions better. For that, we use a gradient descent update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(theta, x, y, lr=0.1):\n",
    "    loss = mse(theta, x, y)\n",
    "    grad_loss = grad(mse)(theta, x, y)\n",
    "    new_params = theta - lr*grad_loss\n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update((1,1),1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, defined a parametric model that we want to model the data with, and defined a loss function, we can train the model. We'll do this in a loop, and plot the loss as a function of the number of training steps.\n",
    "\n",
    "This loop is the 'training loop.' It starts with the definition of an initial point in model space, i.e., an initial value for the parameters theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = jnp.array([5., 1.])\n",
    "\n",
    "num_epochs, losses = 100, []\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss, theta = update(theta, xs, ys)\n",
    "    if epoch % 5 == 0: print(f\"Loss at epoch {epoch} is {loss:.3f}\")\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.scatter(list(range(num_epochs+1)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss looks good. But is it the model we hoped for? Let's check the params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Model: Mapping to Unit Circle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another problem. We want to map a point in 2D to a point on the unit circle. This is a different sort of problem -- there isn't a 'ground truth' label that I'm trying to hit with my predictor. Instead, I'm trying to satisfy a constraint, that my function\n",
    "\n",
    "$f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$\n",
    "\n",
    "maps a point $x$ in the domain onto the unit circle. We'll impose this with a loss function\n",
    "\n",
    "$L = \\sum_i (f(x)\\cdot f(x) - 1)^2$\n",
    "\n",
    "Since $f$ is a model and therefore a function of parameters, $f=f_\\theta$, we want to find \n",
    "\n",
    "$\\theta^* = \\arg \\min_\\theta L(\\theta)$,\n",
    "\n",
    "the value that minimize the loss. \n",
    "\n",
    "Let's start with a linear model again, since it worked last time\n",
    "\n",
    "$f(x) = w \\cdot x$\n",
    "\n",
    "where $w$ and $x$ are now a matrix and vector, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta,x):\n",
    "    w, b = theta\n",
    "    return jnp.einsum('ji,ci->cj', w,x)\n",
    "\n",
    "def unit_circle_loss(theta,x):\n",
    "    fx = model(theta,x) \n",
    "    return jnp.mean((jnp.linalg.norm(fx, axis=1) - 1)**2)\n",
    "\n",
    "def update(theta, x, lr=0.1):\n",
    "    w, b = theta\n",
    "    loss = unit_circle_loss(theta, x)\n",
    "    grad_loss_w, grad_loss_b = grad(unit_circle_loss)(theta, x)\n",
    "    new_params = [w - lr*grad_loss_w, b - lr*grad_loss_b]\n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [jnp.array([[0.5,0.5],[0.5,0.5]]), jnp.array([0.5,0.5])] \n",
    "\n",
    "xs = np.random.normal(size=(num_points,2))\n",
    "\n",
    "num_epochs, losses = 50, []\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss, theta = update(theta, xs)\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.scatter(list(range(num_epochs+1)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see **a little** bit of improvement, but not that much really. We should be able to see that it's not that great with our naked eye!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = model(theta, xs)\n",
    "plt.scatter(fx[:,0], fx[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly not lying on the unit circle! It shouldn't surprise us, because this model is laughably simple and simply shouldn't be able to work for this problem.\n",
    "\n",
    "But what if we use a more powerful model, a non-trivial feedforward neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta, x): \n",
    "    w0, b0, w1, b1 = theta\n",
    "    z = jnp.einsum('ji,ci->cj', w0, x) + b0\n",
    "    z = jnp.maximum(0,z) # ReLU activation function\n",
    "    #z = jnp.tanh(jnp.einsum('ji,ci->cj', w0, x) + b0)\n",
    "    output = jnp.einsum('ji,bi->bj', w1, z) + b1\n",
    "    return output \n",
    "\n",
    "def update(theta, x, lr = 0.001):\n",
    "    w0, b0, w1, b1 = theta\n",
    "    loss = unit_circle_loss(theta, x)\n",
    "    grad_loss_w0, grad_loss_b0, grad_loss_w1, grad_loss_b1 = grad(unit_circle_loss)(theta, x)\n",
    "    new_params = [w0 - lr*grad_loss_w0, b0 - lr*grad_loss_b0, w1 - lr*grad_loss_w1, b1 - lr*grad_loss_b1]\n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "xs = np.random.normal(size=(num_points,2))\n",
    "d, width = xs.shape[1], 1024\n",
    "theta = [np.random.normal(scale=1,size=(width,d)), np.zeros(width), np.random.normal(scale=1,size=(2,width)), np.zeros(2)]\n",
    " \n",
    "plt.scatter(xs[:,0], xs[:,1])\n",
    "plt.show()\n",
    "\n",
    "fx = model(theta, xs)\n",
    "plt.scatter(fx[:,0], fx[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, losses, thetas = 10000, [], []\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss, theta = update(theta,xs)\n",
    "    thetas.append(theta)\n",
    "    losses.append(loss)\n",
    "    if epoch % 1000 == 0:\n",
    "        fx = model(thetas[-1], xs)\n",
    "        plt.scatter(fx[:,0], fx[:,1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: mapping things to the unit circle isnt the most useful thing in the world. But it's a good example of a problem that is hard to solve with a linear model, but easy to solve with a neural network.\n",
    "\n",
    "The point is that this map was *learned from a constraint*, and so you might use neural networks in other situations where you want to learn something satisfying a constraint, e.g. a solution to a diff eq.\n",
    "\n",
    "**Note:** All we plotted was train data. Does it generalize to other random draws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = model(theta, xs)\n",
    "plt.scatter(fx[:,0], fx[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test = np.random.normal(size=(num_points, 2))\n",
    "plt.scatter(xs_test[:,0],xs_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = model(theta, xs_test)\n",
    "plt.scatter(fx[:,0], fx[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it's ok on test points, but not perfect. This is called \"generalization gap\" or \"generalization error.\" It's a measure of how well the model generalizes to new data.\n",
    "\n",
    "In ML, we try to not have generalization gap, by training longer, clever choices, better optimizers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

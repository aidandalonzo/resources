{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Differentiation\n",
    "========================\n",
    "Numeric differentiation is a technique used to approximate the derivative of a function when an analytical derivative is difficult or impossible to obtain. It is widely used to analyze rates of change in discrete data sets or complex functions. The most common methods include finite difference approximations, such as **forward**, **backward**, and **central** differences, which estimate derivatives using function values at nearby points. While numerical differentiation is straightforward to implement, it can introduce errors due to finite precision and step size selection, requiring careful consideration of accuracy and stability.\n",
    "\n",
    "## Forward/Backward Difference \n",
    "\n",
    "The forward/backward difference uses the traditional equation for differentiation:\n",
    "\n",
    "$$\\frac{dy}{dx} = y'(x) =  \\frac{y(x+\\Delta x) - y(x)}{h}$$\n",
    "\n",
    "where $h$ is the **step size**, also denoted as $dx\\approx\\Delta x$.\n",
    "\n",
    "In order to numerically evaluate a derivative $y'(x)=dy/dx$ at point $x_0$, we approximate is by using finite differences.\n",
    "Therefore we find: \n",
    "\n",
    "$dx \\approx \\Delta x =x_1-x_0 = h$\n",
    "\n",
    "$dy \\approx \\Delta y =y_1-y_0= y(x_1)-y(x_0) = y(x_0+\\Delta x)-y(x_0)$\n",
    "\n",
    "Then we re-write the derivative in terms of discrete differences as:\n",
    "$$\\frac{dy}{dx} \\approx \\frac{\\Delta y}{\\Delta x}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's look at the accuracy of this approximation in terms of the interval $\\Delta x$. In our first example we will evaluate the derivative of $y=x^2$ at $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 1.\n",
    "step = []\n",
    "er = []\n",
    "while(dx > 1.e-16):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    step.append(dx)\n",
    "    er.append(d-2)\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 10.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# need absolute values for log plot\n",
    "ers = [abs(x) for x in er ]\n",
    "plt.loglog(step,ers)\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it that the sequence does not converge? This is due to the round-off errors in the representation of the floating point numbers. To see this, we can simply type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((1.+0.0001)*(1+0.0001)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using powers of 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 1.\n",
    "step2 = []\n",
    "er2 = []\n",
    "while(dx > 1.e-16):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    step2.append(dx)\n",
    "    er2.append(d-2)\n",
    "    print(\"%8.5e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need absolute values for log plot\n",
    "ers2 = [abs(x) for x in er2 ]\n",
    "plt.loglog(step2,ers2)\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appeared to have less trouble as the step size got smaller. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central (Midpoint) Difference\n",
    "\n",
    "The central difference method is a more accurate numerical differentiation technique compared to forward or backward differences because it utilizes points on both sides of the target point to estimate the derivative. It is given by the formula:\n",
    "$$ \\frac{dy}{dx} \\approx \\frac{y(x_0+\\frac{h}{2})-y(x_0-\\frac{h}{2})}{h}.$$\n",
    "By averaging symmetric function values around $x$, the central difference method reduces truncation error to the order of $\\mathcal{O}(h^2)$\n",
    "\n",
    "\n",
    "For a more complex function we may need to import it from the math module. For instance, let's calculate the derivative of $sin(x)$ at $x=\\pi/4$, including both the forward and central differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sin, sqrt, pi\n",
    "dx = 1.\n",
    "data = []\n",
    "while(dx > 1.e-16):\n",
    "    x = pi/4.\n",
    "    d1 = sin(x+dx) - sin(x); #forward\n",
    "    d2 = sin(x+dx*0.5) - sin(x-dx*0.5); # midpoint\n",
    "    d1 = d1 / dx;\n",
    "    d2 = d2 / dx;\n",
    "    e1 = d1-sqrt(2.)/2.\n",
    "    e2 = d2-sqrt(2.)/2.\n",
    "    print(\"%8.5e %20.16f %20.16f %20.16f %20.16f\" % (dx, d1, e1, d2, e2) )\n",
    "    data.append([dx,d1,e1,d2,e2])\n",
    "    dx = dx / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arraydata = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? Which one does better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(arraydata[:,0],abs(arraydata[:,2]),'r--')\n",
    "plt.loglog(arraydata[:,0],abs(arraydata[:,4]),'b')\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more in-depth discussion about round-off errors in numerical differentiation can be found <a href=\"http://www.uio.no/studier/emner/matnat/math/MAT-INF1100/h10/kompendiet/kap11.pdf\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special functions in **numpy**\n",
    "\n",
    "numpy provides a simple method **diff()** to calculate the numerical derivatives of a dataset stored in an array by forward differences. The function **gradient()** will calculate the derivatives by midpoint (or central) difference, that provides a more accurate result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y = lambda x: x*x\n",
    "\n",
    "x1 = np.arange(0,10,1)\n",
    "x2 = np.arange(0,10,0.1)\n",
    "\n",
    "y1 = np.gradient(y(x1), 1.)\n",
    "print(y1)\n",
    "print(np.diff(y1))\n",
    "pyplot.plot(x1,np.gradient(y(x1),1.),'r--o');\n",
    "pyplot.plot(x1[:x1.size-1],np.diff(y(x1))/np.diff(x1),'b--x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that **gradient()** uses forward and backward differences at the two ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x2,np.gradient(y(x2)),'b--o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More discussion about numerical differentiation, including higher order methods with error extrapolation can be found <a href=\"http://young.physics.ucsc.edu/115/diff.pdf\">here</a>. \n",
    "\n",
    "## `scipy.differentiate.derivative`\n",
    "\n",
    "The `derivative` function computes numerical derivatives using an adaptive Richardson extrapolation method. It takes a function `f` and array of points `x`, returning a result object with:\n",
    "- `.df` - the derivative values\n",
    "- `.error` - error estimates  \n",
    "- `.success` - convergence flags\n",
    "- `.nit` - number of iterations performed\n",
    "- `.nfev` - number of function evaluations\n",
    "\n",
    "Key parameters include:\n",
    "- `order` - order of finite difference formula (default 8)\n",
    "- `initial_step` - initial step size (default 0.5)\n",
    "- `step_factor` - step reduction factor per iteration (default 2.0)\n",
    "- `tolerances` - dict with 'atol' and 'rtol' keys for convergence criteria\n",
    "- `maxiter` - maximum iterations (default 10)\n",
    "- `step_direction` - 0 for central differences, -1 for backward, +1 for forward\n",
    "\n",
    "The algorithm iteratively refines the derivative estimate by reducing the step size until convergence. Unlike older derivative functions, this automatically adapts and provides error estimates, making it robust for most smooth functions without manual tuning.\n",
    "```python\n",
    "result = derivative(f, x, args=(), tolerances=None, maxiter=10, \n",
    "                   order=8, initial_step=0.5, step_factor=2.0, \n",
    "                   step_direction=0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.differentiate import derivative\n",
    "\n",
    "# Define functions and their analytical derivatives\n",
    "def quadratic(x):\n",
    "    \"\"\"f(x) = x^2\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def dquadratic(x):\n",
    "    \"\"\"f'(x) = 2x\"\"\"\n",
    "    return 2*x\n",
    "\n",
    "def rational(x):\n",
    "    \"\"\"f(x) = 1/(1+x^2)\"\"\"\n",
    "    return 1 / (1 + x**2)\n",
    "\n",
    "def drational(x):\n",
    "    \"\"\"f'(x) = -2x/(1+x^2)^2\"\"\"\n",
    "    return -2*x / (1 + x**2)**2\n",
    "\n",
    "# Evaluate over a domain\n",
    "x = np.linspace(0, 5, 50)\n",
    "\n",
    "# Compute numerical derivatives\n",
    "quad_result = derivative(quadratic, x)\n",
    "rat_result = derivative(rational, x)\n",
    "\n",
    "# Compute analytical derivatives\n",
    "quad_analytical = dquadratic(x)\n",
    "rat_analytical = drational(x)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Quadratic function\n",
    "axes[0, 0].plot(x, quadratic(x), 'b-', linewidth=2)\n",
    "axes[0, 0].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0, 0].set_title('Quadratic: f(x) = x²', fontsize=13)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Quadratic derivative\n",
    "axes[1, 0].plot(x, quad_analytical, 'b-', linewidth=2, label='Analytical')\n",
    "axes[1, 0].plot(x, quad_result.df, 'ro', markersize=4, alpha=0.6, label='Numerical')\n",
    "axes[1, 0].set_xlabel('x', fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"f'(x)\", fontsize=12)\n",
    "axes[1, 0].set_title(\"f'(x) = 2x\", fontsize=13)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rational function\n",
    "axes[0, 1].plot(x, rational(x), 'g-', linewidth=2)\n",
    "axes[0, 1].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0, 1].set_title('Rational: f(x) = 1/(1+x²)', fontsize=13)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Rational derivative\n",
    "axes[1, 1].plot(x, rat_analytical, 'g-', linewidth=2, label='Analytical')\n",
    "axes[1, 1].plot(x, rat_result.df, 'ro', markersize=4, alpha=0.6, label='Numerical')\n",
    "axes[1, 1].set_xlabel('x', fontsize=12)\n",
    "axes[1, 1].set_ylabel(\"f'(x)\", fontsize=12)\n",
    "axes[1, 1].set_title(\"f'(x) = -2x/(1+x²)²\", fontsize=13)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('derivative_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Quadratic function:\")\n",
    "print(f\"  Max absolute error: {np.max(np.abs(quad_result.df - quad_analytical)):.2e}\")\n",
    "print(f\"  Max error estimate: {np.max(quad_result.error):.2e}\")\n",
    "print(f\"  All converged: {np.all(quad_result.success)}\")\n",
    "\n",
    "print(\"\\nRational function:\")\n",
    "print(f\"  Max absolute error: {np.max(np.abs(rat_result.df - rat_analytical)):.2e}\")\n",
    "print(f\"  Max error estimate: {np.max(rat_result.error):.2e}\")\n",
    "print(f\"  All converged: {np.all(rat_result.success)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_result\n",
    "rat_result.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`derivative` returns a dictionary and may have more information than what you want. `gradient` seems to be preferred for numerical data by some sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array representing your function\n",
    "f = np.array([[1, 2, 6], [3, 4, 5]])\n",
    "\n",
    "# Calculate the gradient\n",
    "gradient = np.gradient(f)\n",
    "\n",
    "# gradient is a tuple of two arrays: \n",
    "# - gradient[0]: gradient along the rows (y-axis)\n",
    "# - gradient[1]: gradient along the columns (x-axis)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid size\n",
    "x = np.linspace(-2, 2, 20)  # 20 points from -2 to 2\n",
    "y = np.linspace(-2, 2, 20)\n",
    "\n",
    "# Create meshgrid\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Define function f(x, y) = exp(-x^2 - y^2)\n",
    "F = np.exp(-(X**2 + Y**2))\n",
    "\n",
    "# Compute numerical gradient\n",
    "# Computes gradient with respect to x and y\n",
    "dFdx, dFdy = np.gradient(F, x, y) \n",
    "\n",
    "# Plot function\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.contourf(X, Y, F, levels=20, cmap=\"plasma\")  # Filled contour plot of f(x,y)\n",
    "ax.quiver(X, Y, dFdy, dFdx, color='white')  # Quiver plot of gradient vectors\n",
    "ax.set_title(\"Gradient of a Gaussian Function\")\n",
    "ax.set_xlabel(\"x-axis\")\n",
    "ax.set_ylabel(\"y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the roundoff errors is by simply using the **decimal** package, which provides a higher precision floating point number representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "dx = Decimal(\"1.\")\n",
    "while(dx >= Decimal(\"1.e-10\")):\n",
    "    x = Decimal(\"1.\")\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-Decimal(\"2.\")))\n",
    "    dx = dx / Decimal(\"10.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better than numerical differentiation is automatic differentiation or *autodiff*, which is crucial to breakthroughs in machine learning.\n",
    "\n",
    "This is a technique that allows to evaluate the derivative of a function to machine precision, without the need to use finite differences, using the fact that autodiff package knows the analytical form of the derivative for certain functions. It then builds a computational graph that allows for the evaluation of the derivative of a function using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy\n",
    "`sympy`, the symbolic mathematics library, allows differentiation of expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "f = x**2 + 3*x + 5\n",
    "dfdx = sp.diff(f, x)\n",
    "\n",
    "print(dfdx)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd (may need to install)\n",
    "`autograd` is a Python library that enables automatic differentiation of native Python and NumPy functions. It is lightweight and useful for computing derivatives of functions without needing to manually derive them.\n",
    "Some key features of `autograd`:\n",
    "- Computes gradients automatically.\n",
    "- Works with standard Python and NumPy functions.\n",
    "- Supports higher-order derivatives (e.g., second-order derivatives).\n",
    "- Can differentiate through loops, branches, and recursion.\n",
    "\n",
    "You should think about using `autograd`:\n",
    "- If you need automatic differentiation but don’t want to manually compute derivatives.\n",
    "- If you're working with NumPy-based code (since jax require different frameworks).\n",
    "- If you need higher-order derivatives, like Hessians and Jacobians.\n",
    "\n",
    "Please note some limitations of `autograd`\n",
    "- No support for control flow like if statements depending on values. (JAX solves this.)\n",
    "- Slower than JAX or TensorFlow for large-scale computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd as ag\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 5\n",
    "\n",
    "dfdx = ag.grad(f)  # Get the gradient function\n",
    "print(dfdx(2.0))  # Evaluate derivative at x=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd` can perform partial derivatives, calculate jacobians, and compute hessian matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2 + y**3\n",
    "\n",
    "dfdx = ag.grad(f, argnum=0)  # ∂f/∂x\n",
    "dfdy = ag.grad(f, argnum=1)  # ∂f/∂y\n",
    "print(dfdx)\n",
    "print(dfdx(2.0, 3.0))  \n",
    "print(dfdy(2.0, 3.0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb(x):\n",
    "    return np.array([x[0]**2, x[1]**3])  # Vector-valued function\n",
    "\n",
    "jacob = ag.jacobian(jb)\n",
    "print(jacob(np.array([2.0, 3.0])))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hs(x):\n",
    "    return x[0]**2 + x[1]**3\n",
    "\n",
    "hess = ag.hessian(hs)\n",
    "print(hess(np.array([1.0, 2.0])))  # Output: Hessian matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = jax.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 13.0\n",
    "print(f\"The gradient of f at x = {x} is {grad_f(x):.20f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare this to the finite difference technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 13.\n",
    "while(dx > 1.e-16):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-26.))\n",
    "    dx = dx / 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f(13.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no error on the autodiff result because it knows $dy/dx$ as a *function*, rather than computing it by numerical approximation\n",
    "\n",
    "Ok, so that's great and all, but autodiff really gets its legs when you have a more complicated function. Derivatives of a more complicated function that is composed of many smaller functions require many applications of the chain rule. Autodiff does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_complicated(x):\n",
    "    return jnp.cos(jnp.sin(jnp.tanh(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3.14,100)\n",
    "plt.plot(x,f_complicated(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fcomp = jax.grad(f_complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3.14,100)\n",
    "plt.plot(x,[grad_fcomp(j) for j in x])\n",
    "plt.plot(x,f_complicated(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

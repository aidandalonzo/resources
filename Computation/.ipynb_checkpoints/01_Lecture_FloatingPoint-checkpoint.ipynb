{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2a38ec-8b60-4814-a2a1-99dfcf8f4144",
   "metadata": {},
   "source": [
    "The following borrows heavily from [python.org](https://docs.python.org/3/tutorial/floatingpoint.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab13f0f-789e-4ad6-94cf-006555b1852b",
   "metadata": {},
   "source": [
    "# Floating Point Numbers\n",
    "Computers store floating-point numbers in memory as base 2 (or binary) fractions. Most decimal fractions cannot be represented exactly as binary fractions; therefore, floating-point numbers are only approximations of the actually values they represent. Most users are not aware of the approximation because Python only prints a decimal approximation to the true decimal value of the binary approximation stored by the machine. On most machines, if Python were to print the true decimal value of the binary approximation, it may surprise you.\n",
    "\n",
    "Just as base 10 numbers can be represented as decimal fractions of powers of 10,\n",
    "$$0.125 = \\frac{1}{10} + \\frac{2}{100} + \\frac{5}{1000}$$\n",
    "we can represent numbers in base 2 using fractions of the power of 2,\n",
    "$$0.125 = \\frac{0}{2} + \\frac{0}{4} + \\frac{1}{8}$$\n",
    "Unfortunately, most decimal fractions cannot be represented exactly as binary fractions. We see this in base 10 when considering the fraction $\\frac{1}{3}$,\n",
    "$$\\frac{1}{3} = 0.333333....$$\n",
    "No matter how many 3's we choose to display the number on the right-hand side will never be equivalent to $\\frac{1}{3}$.\n",
    "\n",
    "In the same way, no matter how many base 2 digits you’re willing to use, the decimal value 0.1 cannot be represented exactly as a base 2 fraction. In base 2, $\\frac{1}{10}$ is the infinitely repeating fraction\n",
    "\n",
    "0.0001100110011001100110011001100110011001100110011...\n",
    "\n",
    "Stop at any finite number of bits, and you get an approximation.\n",
    "\n",
    "It’s easy to forget that the stored value is an approximation to the original decimal fraction, because of the way that floats are displayed at the interpreter prompt. Python only prints a decimal approximation to the true decimal value of the binary approximation stored by the machine. If Python were to print the true decimal value of the binary approximation stored for 0.1, it would have to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338b5a7-aff2-463d-9f9e-b6a8dd22427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "format(0.2, '.32f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4190794-9b65-4105-be65-1ead40137a14",
   "metadata": {},
   "source": [
    "That is more digits than most people find useful, so Python keeps the number of digits manageable by displaying a rounded value instead. It’s important to realize that this is, in a real sense, an illusion: the value in the machine is not exactly $\\frac{1}{10}$, you’re simply rounding the display of the true machine value. This fact becomes apparent as soon as you try to do arithmetic with these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94421219-c1f2-470a-aeef-9cb6efa0a9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "0.1+0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2351044-3db1-4640-af52-6d15a874aee9",
   "metadata": {},
   "source": [
    "Note that this is in the very nature of binary floating-point: this is not a bug in Python, and it is not a bug in your code either. You’ll see the same kind of thing in all languages that support your hardware’s floating-point arithmetic (although some languages may not display the difference by default, or in all output modes).\n",
    "\n",
    "Other surprises follow from this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397ed65-596a-40c8-bc0b-9c936e36eaed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(2.675,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58791c-eb97-46a9-ae32-c7e3eb1f4faa",
   "metadata": {},
   "source": [
    "The documentation for the built-in **round()** function says that it rounds to the nearest value, rounding ties away from zero. Since the decimal fraction 2.675 is exactly halfway between 2.67 and 2.68, you might expect the result here to be (a binary approximation to) 2.68. It’s not, because when the decimal string 2.675 is converted to a binary floating-point number, it’s again replaced with a binary approximation, whose exact value is \n",
    "\n",
    "2.67499999999999982236431605997495353221893310546875\n",
    "\n",
    "Since this approximation is slightly closer to 2.67 than to 2.68, it’s rounded down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f371340-5069-4acb-975a-2e296cf07e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61d065f6-12e5-4eff-a746-fda87827814c",
   "metadata": {},
   "source": [
    "## The Perils of Floating Point\n",
    "A famous post [The Perils of Floating Point](http://www.indowsway.com/floatingpoint.htm) by Bruce Bush enumerates the many pitfalls of floating-point arithmetic (mostly in FORTRAN, hey, it was the gold standard at one time). The errors in Python float operations are inherited from floating-point hardware, and on most machines won't exceed more than 1 part in $2^{53}$ per operation. That's more than adequate for most tasks, but you need to keep in mind that it's not decimal arithmetic and that every float operation can suffer a new rounding error.\n",
    "\n",
    "While pathological cases do exist, for the most casual use of floating-point arithmetic you'll see the results you expect in the end if you simply round the display of your final results to the number of decimal digits you expect. Some cases which require exact decimal representations, try using `decimal` module which implements decimal arithmetic suitable for accounting applications and high-precision applications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110383d7-c2fa-4bae-9df6-c34f7119b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code may execute differently in different IDEs\n",
    "import decimal as dc\n",
    "x= 0.10\n",
    "true_x = dc.Decimal(x)\n",
    "print(\"64-bit exact: \", true_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48d7ff-856a-4244-ac45-c7d1252fcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_32 = np.float16(0.1)\n",
    "print(\"32-bit exact:\", format(x_32, \".55f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51cb7f-b13c-4167-a568-f381c2aff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.Decimal(2.675)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03796926-0a2c-4a03-9b96-539cabfc92ed",
   "metadata": {},
   "source": [
    "Another form of exact arithmetic is supported by the `fractions` module which implements arithmetic based on rational numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bcc3f-adff-48f1-95d4-504580cc1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "# Example floating point number\n",
    "num = 0.19755\n",
    "\n",
    "# Convert to a fraction\n",
    "frac = Fraction(num).limit_denominator()\n",
    "\n",
    "# Show approximation in base 2 representation\n",
    "binary_rep = frac.limit_denominator(2**9)  # Limit denominator to power of 2\n",
    "\n",
    "print(f\"Floating point number: {num}\")\n",
    "print(f\"Fraction approximation: {frac}\")\n",
    "print(f\"Binary approximation (base-2 denominator): {binary_rep}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476db0f-a665-479c-9cfa-47f4f0a8899f",
   "metadata": {},
   "source": [
    "Base Python even provides tools that may help on those rare occasions when you really do want to know exact values of a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da080f21-e50c-42e6-bb5c-b5a23a4be2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fcb26-048b-496d-9920-2771919fee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "float.fromhex(x.hex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904c93f-2f1a-4977-9205-7f0a7dd55e16",
   "metadata": {},
   "source": [
    "## Floating-Point Notation\n",
    "Floating-point numbers are represented in IEEE-754 standard notations that consists of 3 fields:\n",
    "\n",
    "- sign\n",
    "- mantissa\n",
    "- exponent\n",
    "\n",
    "Each field contains information about the number which is represented in the form,\n",
    "\n",
    "$$(-1)^s \\times 1.f\\times 2^{e-bias}$$\n",
    "\n",
    "The bias exists so that numbers can be compared by the same hardware that compares signed integers. The bias takes a value of 127 for 32-bit numbers and 1023 for 64-bit numbers. The leading 1 is part of the \"normalization\", and is assumed and not stored, saving a bit...of memory. Depending whether the float is 32-bit or 64-bit, each field occupies a different amount of space.\n",
    "\n",
    "<img src=\"figures/IEEE_754_Single_Floating_Point_Format.png\">\n",
    "<img src=\"figures/IEEE_754_Double_Floating_Point_Format.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e4ec2-5b76-4bdd-9705-ab3e9f74dc03",
   "metadata": {},
   "source": [
    "### Manual Algorithm to Transform Numbers to Floating Point\n",
    "You can directly convert any number into floating point notation (32- or 64-bit) using the following algorithm\n",
    "1. Normalize the number into $1.M×2^E$\n",
    "2. Determine the sign bit (0 for positive, 1 for negative)\n",
    "3. Calculate the biased exponent ($E+127$ or $E+1023$)\n",
    "4. Extract and convert the fractional part of the mantissa to binary (there is a quick and seeming simple way of doing this)\n",
    "5. Assemble the final 32-bit representation.\n",
    "\n",
    "Of course, this seems like unnecessary work, especially since the notation is strictly used to store data on computers - let the computer do the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f52d32-865c-4d67-a98f-d15731ebe232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code requires conversion.py\n",
    "import conversion as cvn\n",
    "x = 0.15625\n",
    "x32 = cvn.num2float32(x)\n",
    "x64 = cvn.num2float64(x)\n",
    "print(f'{x32}\\n{x64}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24def5e2-a7ff-42c9-8b21-82738c99dc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58236962-1f8b-4935-bf99-63af8c7ff866",
   "metadata": {},
   "source": [
    "## Special Cases\n",
    "### Zero \n",
    "IEEE-754 does make a distinction between $\\pm 0$ with the sign bit. The exponent and mantissa bits are all set to zero.\n",
    "### Infinity\n",
    "IEEE-754 does represent $\\pm \\infty$ with the sign bit. The exponent bits are all set to 1, while the mantissa is set to all zeros\n",
    "### NaN\n",
    "IEEE-754 has two types of NaNs. The \"quiet\" NaNs have an insignificant sign bit, exponent bits set to 1 and the mantissa '10000....'. The \"signaling\" NaNs also have an insignificant sign bit, exponent all exponent bits set to 1, and the mantissa '000....(something nonzero)'. The signaling NaN exists to trip an exception in execution. However, any distinction between silent and signaling NaNs is useless in Python.\n",
    "\n",
    "NaNs arise when something goes wrong with a calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6894429-f8ab-4ef7-8d66-e969533bc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xNaN = np.float64(NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182aba3-fe07-4e33-86d3-3ace1609a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Common sources of NaN in physics simulations:\n",
    "examples = {\n",
    "    '∞ - ∞': np.inf - np.inf,\n",
    "    '0 * ∞': 0.0 * np.inf,\n",
    "    'sqrt(-1)': np.sqrt(-1.0),  # Warns, returns NaN\n",
    "    'log(-1)': np.log(-1.0),    # Warns, returns NaN\n",
    "    'arcsin(2)': np.arcsin(2.0), # Domain error\n",
    "}\n",
    "\n",
    "for expr, result in examples.items():\n",
    "    print(f\"{expr:12s} = {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf24ad-a609-4b2a-ab64-7effb02ae304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pure Python float division:\")\n",
    "try:\n",
    "    result = 0.0 / 0.0\n",
    "    print(f\"0.0 / 0.0 = {result}\")\n",
    "except ZeroDivisionError as e:\n",
    "    print(f\"0.0 / 0.0 raises: {e}\")\n",
    "\n",
    "print(\"\\nNumPy scalar division:\")\n",
    "result = np.float64(0.0) / np.float64(0.0)\n",
    "print(f\"np.float64(0.0) / np.float64(0.0) = {result}\")\n",
    "\n",
    "print(\"\\nNumPy array division:\")\n",
    "result = np.array([0.0]) / np.array([0.0])\n",
    "print(f\"np.array([0.0]) / np.array([0.0]) = {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9b119-efd0-468f-ae2e-3434b2ea564f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
